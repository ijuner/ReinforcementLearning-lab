{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Setup and Dependencies\n",
    "\n",
    "# Install dependencies if needed (uncomment the next line)\n",
    "# !pip install gym torch numpy matplotlib\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Define the Neural Network\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Initialize the Environment\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Set up DQN\n",
    "policy_net = DQN(state_dim, action_dim)\n",
    "target_net = DQN(state_dim, action_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "gamma = 0.99  # Discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Preprocess the Input\n",
    "\n",
    "def preprocess_state(state):\n",
    "    return torch.tensor(state, dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 5: Experience Replay Buffer\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "def add_experience(state, action, reward, next_state, done):\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "def sample_experiences(batch_size=32):\n",
    "    return random.sample(replay_buffer, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Step 6: Train the DQN\n",
    "\n",
    "def train(batch_size=32):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    \n",
    "    batch = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    states = torch.cat([preprocess_state(s) for s in states])\n",
    "    actions = torch.tensor(actions).unsqueeze(1)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "    next_states = torch.cat([preprocess_state(s) for s in next_states])\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    q_values = policy_net(states).gather(1, actions)\n",
    "    next_q_values = target_net(next_states).max(1, keepdim=True)[0].detach()\n",
    "    target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "    \n",
    "    loss = criterion(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 7: Train the Agent\n",
    "\n",
    "num_episodes = 500\n",
    "batch_size = 32\n",
    "sync_target_steps = 10\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = state[0] if isinstance(state, tuple) else state\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(policy_net(preprocess_state(state))).item()\n",
    "        \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        add_experience(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        train(batch_size)\n",
    "    \n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    episode_rewards.append(total_reward)\n",
    "    \n",
    "    if episode % sync_target_steps == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}: Reward = {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 8: Visualize the Results\n",
    "\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training Progress')\n",
    "plt.show()\n",
    "\n",
    "### Step 9: Play the Game with the Trained Model\n",
    "\n",
    "def play():\n",
    "    state = env.reset()\n",
    "    state = state[0] if isinstance(state, tuple) else state\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        with torch.no_grad():\n",
    "            action = torch.argmax(policy_net(preprocess_state(state))).item()\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    env.close()\n",
    "    print(f'Total Reward: {total_reward}')\n",
    "\n",
    "play()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
