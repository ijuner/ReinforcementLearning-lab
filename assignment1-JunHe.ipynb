{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Programming - CSCN 8020 - Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Jun He (8903073)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "### **Pick-and-Place Robot**: Consider using reinforcement learning to control the motion of a robot arm in a repetitive pick-and-place task. If we want to learn movements that are fast and smooth, the learning agent will have to control the motors directly and obtain feedback about the current positions and velocities of the mechanical linkages.Design the reinforcement learning problem as an MDP, define states, actions, rewards with reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The states represent different stages in the pick-and-place task\n",
    "states = [\"idle\",\"picking\",\"placing\",\"finished\"]\n",
    "# The actions define the transitions between the states.\n",
    "actions = [\"move-pick\",\"pick\",\"move-place\",\"place\"]\n",
    "# The rewards encourage the agent to complete the task efficiently.\n",
    "rewards = {\"picking\":10, \"placing\":20, \"finished\":100}\n",
    "# Define transition probabilities\n",
    "transitions = {\n",
    "    \"idle\":{\"move-pick\":\"picking\"},\n",
    "    \"picking\":{\"pick\":\"placing\"},\n",
    "    \"placing\":{\"move-place\":\"finished\"},\n",
    "    \"finished\":{}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: pick, New State: placing, Reward: 20\n",
      "Action: move-place, New State: finished, Reward: 100\n"
     ]
    }
   ],
   "source": [
    "state = \"picking\"\n",
    "while state != \"finished\":\n",
    "    action = list(transitions[state].keys())[0]  \n",
    "    # Taking the first available action\n",
    "    ## get next state\n",
    "    state = transitions.get(state,{}).get(action, state)\n",
    "    ## get reward\n",
    "    reward = rewards.get(state, 0)\n",
    "    print(f\"Action: {action}, New State: {state}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "### 2x2 Gridworld: Consider a 2x2 gridworld with the following characteristics: \n",
    "* • State Space (S): s1, s2, s3, s4.\n",
    "* • Action Space (A): up, down, left, right.\n",
    "* • Initial Policy (π): For all states, π(up|s) = 1.\n",
    "* • Transition Probabilities P (s′|s, a):\n",
    "* – If the action is valid (does not run into a wall), the transition is deterministic.\n",
    "* – Otherwise, s′ = s.\n",
    "* • Rewards R(s):\n",
    "* – R(s1) = 5 for all actions a.\n",
    "– R(s2) = 10 for all actions a.\n",
    "– R(s3) = 1 for all actions a.\n",
    "– R(s4) = 2 for all actions a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "Perform two iterations of Value Iteration for this gridworld environment. Show the step-by-step process\n",
    "(without code) including policy evaluation and policy improvement. Provide the following for each\n",
    "iteration:\n",
    "* • Iteration 1:\n",
    "1. Show the initial value function (V) for each state.\n",
    "2. Perform value function updates.\n",
    "3. Show the updated value function.\n",
    "* • Iteration 2: Show the value function (V) after the second iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---declare each item need  <br>\n",
    "--- **State** Space<br>\n",
    "states = [s<sub>1</sub>,s<sub>2</sub>,s<sub>3</sub>,s<sub>4</sub>]<br>\n",
    "\n",
    "\n",
    "--- The **actions** <br>\n",
    "actions = [\"Up\",\"Down\",\"Left\",\"Right\"]<br>\n",
    "\n",
    "--- The **rewards** <br>\n",
    "R(s<sub>1</sub>)=5\n",
    "R(s<sub>2</sub>)=10\n",
    "R(s<sub>3</sub>)=1\n",
    "R(s<sub>4</sub>)=2\n",
    "\n",
    "--- The **Discount factor** <br>\n",
    "gamma = 0.9\n",
    "\n",
    "--- The **Transition Dynamics** <br>\n",
    "* If an action moves to a valid state → deterministic transition.\n",
    "* If an action hits a wall → stays in the same state.\n",
    "\n",
    "--- The **initializing the value function** <br>\n",
    "V<sub>0</sub>(s<sub>1</sub>) = 0<br>\n",
    "V<sub>0</sub>(s<sub>2</sub>) = 0<br>\n",
    "V<sub>0</sub>(s<sub>3</sub>) = 0<br>\n",
    "V<sub>0</sub>(s<sub>4</sub>) = 0<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iteration 1\n",
    "--- **update the value function for each state using the Bellman equation**<br>\n",
    "there are several possible states: <br>\n",
    "**State 1** <br>\n",
    "Right : V(s<sub>2</sub>) = 0<br>\n",
    "Down : V(s<sub>3</sub>) = 0<br>\n",
    "Left and Up hit walls (stay in s<sub>1</sub>): V(s<sub>1</sub>) = 0<br>\n",
    "so: V<sub>1</sub>(s<sub>1</sub>) = 5+0.9×0=5 <br>\n",
    "\n",
    "**State 2** <br>\n",
    "Left  : V(s<sub>1</sub>) = 0<br>\n",
    "Down : V(s<sub>4</sub>) = 0<br>\n",
    "Right and Up hit walls (stay in s<sub>1</sub>): V(s<sub>1</sub>) = 0<br>\n",
    "so: V<sub>1</sub>(s<sub>2</sub>) = 10+0.9×0=10 <br>\n",
    "\n",
    "**State 3** <br>\n",
    "Right : V(s<sub>4</sub>) = 0<br>\n",
    "Up  : V(s<sub>1</sub>) = 0<br>\n",
    "Left and Up hit walls (stay in s<sub>1</sub>): V(s<sub>1</sub>) = 0<br>\n",
    "so: V<sub>1</sub>(s<sub>3</sub>) = 1+0.9×0=1 <br>\n",
    "\n",
    "**State 4** <br>\n",
    "Left  : V(s<sub>3</sub>) = 0<br>\n",
    "Up  : V(s<sub>2</sub>) = 0<br>\n",
    "Left and Up hit walls (stay in s<sub>1</sub>): V(s<sub>1</sub>) = 0<br>\n",
    "so: V<sub>1</sub>(s<sub>4</sub>) = 2+0.9×0=2 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iteration 2 （we update the values again using the new values from Iteration 1.）<br>\n",
    "there are several possible states: <br>\n",
    "**State 1** <br>\n",
    "Right : V<sub>1</sub>(s<sub>2</sub>) = 10<br>\n",
    "Down : V<sub>1</sub>(s<sub>3</sub>) = 1<br>\n",
    "\n",
    "so: V<sub>2</sub>(s<sub>1</sub>) = 5+0.9×10=14 <br>\n",
    "\n",
    "**State 2** <br>\n",
    "Left  : V<sub>1</sub>(s<sub>1</sub>) = 5<br>\n",
    "Down : V<sub>1</sub>(s<sub>4</sub>) = 2<br>\n",
    "\n",
    "so: V<sub>2</sub>(s<sub>2</sub>) = 10+0.9×5=14.5<br>\n",
    "\n",
    "**State 3** <br>\n",
    "Right : V<sub>1</sub>(s<sub>4</sub>) = 0<br>\n",
    "Up  : V<sub>1</sub>(s<sub>1</sub>) = 0<br>\n",
    "\n",
    "so: V<sub>2</sub>(s<sub>3</sub>) = 1+0.9×5=5.5 <br>\n",
    "\n",
    "**State 4** <br>\n",
    "Left  : V<sub>1</sub>(s<sub>3</sub>) = 0<br>\n",
    "Up  : V<sub>1</sub>(s<sub>2</sub>) = 0<br>\n",
    "\n",
    "so: V<sub>2</sub>(s<sub>4</sub>) = 2+0.9×10=11 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gamma = 0.9  # Discount factor\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0}  # Initial values\n",
    "R = {\"s1\": 5, \"s2\": 10, \"s3\": 1, \"s4\": 2}  # Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Bellman’s optimality equation to calculate value function for each state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(2):\n",
    "#     new_V = {}\n",
    "#     for s in V:\n",
    "#         new_V[s] = R[s] + gamma * max([V[s] for a in [\"up\", \"down\", \"left\", \"right\"]])\n",
    "#     V = new_V\n",
    "#     print(f\"Iteration {_+1} Value Function: {V}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "### 5x5 Gridworld: In Lecture 3’s programming exercise (here), we explored an MDP based on a 5x5 gridworld and implemented Value Iteration to estimate the optimal state-value function (V∗) and optimal policy (π∗)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "#### Task1: Update MDP Code\n",
    "1. Update the reward function to be a list of reward based on whether the state is terminal, grey,\n",
    "or a regular state.\n",
    "2. Run the existing code developed in class and obtain the optimal state-values and optimal policy.\n",
    "Provide a figures of the gridworld with the obtained V∗ and π∗ (You can manually create a table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1, -1, -5],\n",
       "       [-1, -1, -5, -1, -1],\n",
       "       [-1, -1, -1, -1, -1],\n",
       "       [-5, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, 10]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a larger gridworld for more complex reinforcement learning scenarios\n",
    "size = 5\n",
    "V_grid = np.zeros((size, size))  # Initialize value function grid\n",
    "policy = np.full((size, size), \" \")  # Initialize policy grid\n",
    "\n",
    "# Define rewards\n",
    "R_grid = np.full((size, size), -1)  # Default reward for non-terminal states\n",
    "R_grid[4, 4] = 10  # Goal state reward\n",
    "R_grid[1, 2] = R_grid[3, 0] = R_grid[0, 4] = -5  # Grey states (unfavorable regions)\n",
    "R_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(grid, R, gamma=0.9, theta=1e-3):\n",
    "    \"\"\"Performs value iteration on the given grid.\"\"\"\n",
    "    V = np.zeros_like(grid, dtype=float)\n",
    "    delta = float(\"inf\")\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for i in range(grid.shape[0]):\n",
    "            for j in range(grid.shape[1]):\n",
    "                v = V[i, j]  # Store old value\n",
    "                # Get best possible action value based on neighbors\n",
    "                possible_values = []\n",
    "                if i > 0:\n",
    "                    possible_values.append(V[i-1, j])  # Up\n",
    "                if i < size-1:\n",
    "                    possible_values.append(V[i+1, j])  # Down\n",
    "                if j > 0:\n",
    "                    possible_values.append(V[i, j-1])  # Left\n",
    "                if j < size-1:\n",
    "                    possible_values.append(V[i, j+1])  # Right\n",
    "                \n",
    "                V[i, j] = R[i, j] + gamma * max(possible_values, default=0)  # Apply Bellman equation\n",
    "                delta = max(delta, abs(v - V[i, j]))  # Compute update size\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function:\n",
      "[[14.91813735 17.68758951 20.76468156 24.18360341 23.98234307]\n",
      " [17.68758951 20.76468156 20.18360341 27.98234307 32.20310876]\n",
      " [20.76468156 24.18360341 27.98234307 32.20310876 36.89279788]\n",
      " [20.18360341 27.98234307 32.20310876 36.89279788 42.10351809]\n",
      " [27.98234307 32.20310876 36.89279788 42.10351809 47.89316629]]\n"
     ]
    }
   ],
   "source": [
    "V_grid = value_iteration(V_grid, R_grid)\n",
    "print(\"Optimal Value Function:\")\n",
    "print(V_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Policy Extraction\n",
    "def extract_policy(V, size):\n",
    "    \"\"\"Extracts the optimal policy from the optimal value function.\"\"\"\n",
    "    policy = np.full((size, size), \" \")\n",
    "    actions = [\"↑\", \"↓\", \"←\", \"→\"]\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            best_action = None\n",
    "            best_value = -float(\"inf\")\n",
    "            for action, (di, dj) in zip(actions, directions):\n",
    "                ni, nj = i + di, j + dj\n",
    "                if 0 <= ni < size and 0 <= nj < size:\n",
    "                    if V[ni, nj] > best_value:\n",
    "                        best_value = V[ni, nj]\n",
    "                        best_action = action\n",
    "            if best_action:\n",
    "                policy[i, j] = best_action\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "[['↓' '↓' '→' '↓' '↓']\n",
      " ['↓' '↓' '↓' '↓' '↓']\n",
      " ['→' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↑']]\n"
     ]
    }
   ],
   "source": [
    "# Extract and Display Optimal Policy\n",
    "optimal_policy = extract_policy(V_grid, size)\n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Value Iteration Variations\n",
    "Implement the following variation of value iteration. Confirm that it reaches the same optimal state-\n",
    "value function and policy.\n",
    "1. In-Place Value Iteration: Use a single array to store the state values. This means that you\n",
    "update the value of a state and immediately use that updated value in the subsequent updates.\n",
    "Deliverables\n",
    "* • Full code with comments to explain key steps and calculations.\n",
    "* • Provide the estimated value function for each state.\n",
    "* • Important: Compare the performance of these variations in terms of optimization time, number of episodes, and provide comments on their computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_place_value_iteration(grid, R, gamma=0.9, theta=1e-3):\n",
    "    \"\"\"Performs in-place value iteration using a single array to store state values.\"\"\"\n",
    "    V = np.zeros_like(grid, dtype=float)  # Initialize value function\n",
    "    delta = float(\"inf\")\n",
    "    \n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for i in range(grid.shape[0]):\n",
    "            for j in range(grid.shape[1]):\n",
    "                v_old = V[i, j]  # Store old value before update\n",
    "                \n",
    "                # Get best possible action value based on neighbors\n",
    "                possible_values = []\n",
    "                if i > 0:\n",
    "                    possible_values.append(V[i-1, j])  # Up\n",
    "                if i < size-1:\n",
    "                    possible_values.append(V[i+1, j])  # Down\n",
    "                if j > 0:\n",
    "                    possible_values.append(V[i, j-1])  # Left\n",
    "                if j < size-1:\n",
    "                    possible_values.append(V[i, j+1])  # Right\n",
    "                \n",
    "                # Apply Bellman equation in-place\n",
    "                V[i, j] = R[i, j] + gamma * max(possible_values, default=0)\n",
    "                \n",
    "                # Compute maximum change in values\n",
    "                delta = max(delta, abs(v_old - V[i, j]))\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Place Optimal Value Function:\n",
      "[[14.91813735 17.68758951 20.76468156 24.18360341 23.98234307]\n",
      " [17.68758951 20.76468156 20.18360341 27.98234307 32.20310876]\n",
      " [20.76468156 24.18360341 27.98234307 32.20310876 36.89279788]\n",
      " [20.18360341 27.98234307 32.20310876 36.89279788 42.10351809]\n",
      " [27.98234307 32.20310876 36.89279788 42.10351809 47.89316629]]\n"
     ]
    }
   ],
   "source": [
    "# Run In-Place Value Iteration\n",
    "V_in_place = in_place_value_iteration(V_grid, R_grid)\n",
    "print(\"In-Place Optimal Value Function:\")\n",
    "print(V_in_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Policy Extraction\n",
    "def extract_policy(V, size):\n",
    "    \"\"\"Extracts the optimal policy from the optimal value function.\"\"\"\n",
    "    policy = np.full((size, size), \" \")\n",
    "    actions = [\"↑\", \"↓\", \"←\", \"→\"]\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            best_action = None\n",
    "            best_value = -float(\"inf\")\n",
    "            for action, (di, dj) in zip(actions, directions):\n",
    "                ni, nj = i + di, j + dj\n",
    "                if 0 <= ni < size and 0 <= nj < size:\n",
    "                    if V[ni, nj] > best_value:\n",
    "                        best_value = V[ni, nj]\n",
    "                        best_action = action\n",
    "            if best_action:\n",
    "                policy[i, j] = best_action\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (In-Place):\n",
      "[['↓' '↓' '→' '↓' '↓']\n",
      " ['↓' '↓' '↓' '↓' '↓']\n",
      " ['→' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↑']]\n"
     ]
    }
   ],
   "source": [
    "# Extract and Display Optimal Policy for In-Place Value Iteration\n",
    "optimal_policy_in_place = extract_policy(V_in_place, size)\n",
    "print(\"Optimal Policy (In-Place):\")\n",
    "print(optimal_policy_in_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1738884741.1450758\n",
      "1738884741.1470912\n"
     ]
    }
   ],
   "source": [
    "# Comparison\n",
    "import time\n",
    "#general iteration\n",
    "start_v = time.time()\n",
    "V_standard = value_iteration(V_grid, R_grid)\n",
    "end_v = time.time()\n",
    "print(end_v)\n",
    "\n",
    "#i-place iteration\n",
    "start_iv = time.time()\n",
    "V_in_place = in_place_value_iteration(V_grid, R_grid)\n",
    "end_iv = time.time()\n",
    "print(end_iv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "### Off-policy Monte Carlo with Importance Sampling: We will use the same environment, states,actions, and rewards in Problem 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Implement the off-policy Monte Carlo with Importance sampling algorithm to estimate the value function for the given gridworld. Use a fixed behavior policy b(a|s) (e.g., a random policy) to generate episodes and a greedy target policy.<br>\n",
    "**Suggested steps**\n",
    "1. Generate multiple episodes using the behavior policy b(a|s).\n",
    "2. For each episode, calculate the returns (sum of discounted rewards) for each state.\n",
    "3. Use importance sampling to estimate the value function and update the target policy π(a|s).\n",
    "4. You can assume a specific discount factor (e.g., γ = 0.9) for this problem.\n",
    "5. Use the same main algorithm implemented in lecture 4 in class.<br>\n",
    "**Deliverables**\n",
    "* • Full code with comments to explain key steps and calculations.\n",
    "* • Provide the estimated value function for each state.\n",
    "* • Important Compare the estimated value function obtained from Monte Carlo with the one\n",
    "obtained from Value Iteration in terms of optimization time, number of episodes, computational complexity, and any other aspects you notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment parameters for the 5x5 Gridworld\n",
    "size = 5\n",
    " # Initialize value function grid\n",
    "V_mc = np.zeros((size, size)) \n",
    "# Initialize policy grid\n",
    "policy = np.full((size, size), \" \")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(behavior_policy, grid_size, gamma=0.9):\n",
    "    \"\"\"Generates an episode using the behavior policy (random policy).\"\"\"\n",
    "    episode = []\n",
    "    state = (random.randint(0, grid_size-1), random.randint(0, grid_size-1))  # Start at a random state\n",
    "    while state != (4, 4):  # Until goal state is reached\n",
    "        action = random.choice([\"up\", \"down\", \"left\", \"right\"])  # Choose random action\n",
    "        next_state = (max(0, min(grid_size-1, state[0] + (action == \"down\") - (action == \"up\"))),\n",
    "                      max(0, min(grid_size-1, state[1] + (action == \"right\") - (action == \"left\"))))\n",
    "        reward = -1 if state not in [(1,2), (3,0), (0,4), (4,4)] else (-5 if state in [(1,2), (3,0), (0,4)] else 10)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_importance_sampling(grid_size, episodes=1000, gamma=0.9):\n",
    "    \"\"\"Implements off-policy Monte Carlo learning with importance sampling.\"\"\"\n",
    "    returns = {s: [] for s in [(i, j) for i in range(grid_size) for j in range(grid_size)]}\n",
    "    V_mc = np.zeros((grid_size, grid_size), dtype=float)\n",
    "    C = np.zeros((grid_size, grid_size), dtype=float)  # Cumulative sum of importance weights\n",
    "    target_policy = np.full((grid_size, grid_size), \" \")\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        episode = generate_episode(policy, grid_size, gamma)\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            C[state] += W\n",
    "            V_mc[state] += (W / C[state]) * (G - V_mc[state])\n",
    "            if action != \"greedy\":  # Stop if action is not in greedy policy\n",
    "                break\n",
    "            W *= 1 / 0.25  # Update importance weight (assuming behavior policy is uniform random over 4 actions)\n",
    "    \n",
    "    return V_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Estimated Value Function:\n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Run Monte Carlo with Importance Sampling\n",
    "V_mc_importance = monte_carlo_importance_sampling(size)\n",
    "print(\"Monte Carlo Estimated Value Function:\")\n",
    "print(V_mc_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1738886455.5600617\n",
      "1738886455.6151958\n"
     ]
    }
   ],
   "source": [
    "# compare the estimated value function obtained from Monte Carlo with the one obtained from Value Iteration in terms of optimization time\n",
    "start_vi = time.time()\n",
    "V_vi = value_iteration(V_grid, R_grid)\n",
    "end_vi = time.time()\n",
    "print(end_vi)\n",
    "\n",
    "start_mc = time.time()\n",
    "V_mc_importance = monte_carlo_importance_sampling(size)\n",
    "end_mc = time.time()\n",
    "print(end_mc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Helena_AI_Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
